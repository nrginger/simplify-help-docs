{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b63773-1fbf-4eb6-bba0-9f348947da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc652552-1e4a-4e6d-a93e-1608503c8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасета\n",
    "df = pd.read_csv('finetuning_text_pairs_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b2dfa8f-bb9d-4548-bbe6-df063c605c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разметка оригинальных текстов...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5b2a192fe84bf2886dd1492dbf4bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2867 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разметка перефразированных текстов...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a47b8a41a2743888f43633967a64bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2867 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разметка завершена. Результаты сохранены в 'annotated_text_pairs.json'\n"
     ]
    }
   ],
   "source": [
    "# Загрузка модели spaCy для русского языка\n",
    "nlp = spacy.load('ru_core_news_md')\n",
    "\n",
    "# Функция для разметки текста и извлечения признаков\n",
    "def annotate_text(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Создание структуры для хранения разметки\n",
    "    annotation = {\n",
    "        'text': text,\n",
    "        'tokens': [],\n",
    "        'sentences': [],\n",
    "        'entities': [],\n",
    "        'text_stats': {\n",
    "            'n_tokens': len(doc),\n",
    "            'n_sentences': len(list(doc.sents)),\n",
    "            'avg_token_length': sum(len(token.text) for token in doc) / len(doc) if len(doc) > 0 else 0,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Добавление информации о токенах\n",
    "    for token in doc:\n",
    "        annotation['tokens'].append({\n",
    "            'text': token.text,\n",
    "            'lemma': token.lemma_,\n",
    "            'pos': token.pos_,\n",
    "            'tag': token.tag_,\n",
    "            'dep': token.dep_,\n",
    "            'is_stop': token.is_stop,\n",
    "            'is_punct': token.is_punct,\n",
    "            'is_digit': token.is_digit,\n",
    "        })\n",
    "    \n",
    "    # Добавление информации о предложениях\n",
    "    for sent in doc.sents:\n",
    "        annotation['sentences'].append({\n",
    "            'text': sent.text,\n",
    "            'start_char': sent.start_char,\n",
    "            'end_char': sent.end_char,\n",
    "            'n_tokens': len(sent),\n",
    "        })\n",
    "    \n",
    "    # Добавление информации о сущностях\n",
    "    for ent in doc.ents:\n",
    "        annotation['entities'].append({\n",
    "            'text': ent.text,\n",
    "            'start_char': ent.start_char,\n",
    "            'end_char': ent.end_char,\n",
    "            'label': ent.label_,\n",
    "        })\n",
    "    \n",
    "    return annotation\n",
    "\n",
    "# Выполнение разметки с индикатором прогресса\n",
    "print(\"Разметка оригинальных текстов...\")\n",
    "original_annotations = []\n",
    "for text in tqdm(df['original_text']):\n",
    "    original_annotations.append(annotate_text(text))\n",
    "\n",
    "print(\"Разметка перефразированных текстов...\")\n",
    "paraphrased_annotations = []\n",
    "for text in tqdm(df['paraphrased_text']):\n",
    "    paraphrased_annotations.append(annotate_text(text))\n",
    "\n",
    "# Создание итогового датасета\n",
    "annotated_data = []\n",
    "for i in range(len(df)):\n",
    "    annotated_data.append({\n",
    "        'original': original_annotations[i],\n",
    "        'paraphrased': paraphrased_annotations[i]\n",
    "    })\n",
    "\n",
    "# Сохранение результатов в JSON\n",
    "with open('annotated_text_pairs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(annotated_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Разметка завершена. Результаты сохранены в 'annotated_text_pairs.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943bc8e-33f2-4d27-a291-2b3223b0c43a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
